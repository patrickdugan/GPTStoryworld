{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interactive Storyworld Training Notebook\n",
    "\n",
    "This notebook provides an interactive interface for training and analyzing the SAE + RL system.\n",
    "\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# Import our modules\n",
    "from sae_narrative_features import (\n",
    "    SparseAutoencoder,\n",
    "    StoryWorldStateExtractor,\n",
    "    train_sae_on_rollouts,\n",
    "    FeatureAffordanceAnalyzer\n",
    ")\n",
    "\n",
    "from rl_training_infrastructure import (\n",
    "    StoryWorldVerifiers,\n",
    "    StoryWorldRLTrainer,\n",
    "    RLConfig\n",
    ")\n",
    "\n",
    "from integrated_training_pipeline import (\n",
    "    IterativeTrainingPipeline,\n",
    "    FeatureAwareVerifiers\n",
    ")\n",
    "\n",
    "print(\"Imports successful!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Device: {'GPU' if torch.cuda.is_available() else 'CPU'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Generate Synthetic Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load synthetic generation functions from quickstart_demo\n",
    "from quickstart_demo import generate_synthetic_storyworld, generate_synthetic_rollout\n",
    "\n",
    "# Generate storyworlds\n",
    "n_storyworlds = 20\n",
    "storyworlds = [generate_synthetic_storyworld() for _ in range(n_storyworlds)]\n",
    "\n",
    "print(f\"Generated {len(storyworlds)} storyworlds\")\n",
    "print(f\"\\nExample storyworld structure:\")\n",
    "print(json.dumps(storyworlds[0], indent=2)[:500] + \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate rollouts\n",
    "rollouts = []\n",
    "for sw in storyworlds:\n",
    "    for _ in range(5):  # 5 rollouts per storyworld\n",
    "        rollout = generate_synthetic_rollout(sw)\n",
    "        if len(rollout) > 0:\n",
    "            rollouts.append(rollout)\n",
    "\n",
    "print(f\"Generated {len(rollouts)} rollouts\")\n",
    "print(f\"Average rollout length: {np.mean([len(r) for r in rollouts]):.1f} steps\")\n",
    "\n",
    "# Visualize rollout length distribution\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.hist([len(r) for r in rollouts], bins=20, edgecolor='black')\n",
    "plt.xlabel('Rollout Length (steps)')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Distribution of Rollout Lengths')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Train Sparse Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train SAE\n",
    "sae, dataset, history = train_sae_on_rollouts(\n",
    "    rollouts,\n",
    "    latent_dim=128,\n",
    "    sparsity_coef=0.05,\n",
    "    n_epochs=30,\n",
    "    batch_size=16\n",
    ")\n",
    "\n",
    "print(\"\\nTraining complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize training curves\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
    "\n",
    "axes[0, 0].plot(history['total_loss'])\n",
    "axes[0, 0].set_title('Total Loss')\n",
    "axes[0, 0].set_xlabel('Epoch')\n",
    "axes[0, 0].grid(True)\n",
    "\n",
    "axes[0, 1].plot(history['mse_loss'])\n",
    "axes[0, 1].set_title('Reconstruction Loss (MSE)')\n",
    "axes[0, 1].set_xlabel('Epoch')\n",
    "axes[0, 1].grid(True)\n",
    "\n",
    "axes[1, 0].plot(history['sparsity_loss'])\n",
    "axes[1, 0].set_title('Sparsity Loss (L1)')\n",
    "axes[1, 0].set_xlabel('Epoch')\n",
    "axes[1, 0].grid(True)\n",
    "\n",
    "axes[1, 1].plot(history['l0_norm'])\n",
    "axes[1, 1].set_title('Feature Sparsity (L0)')\n",
    "axes[1, 1].set_xlabel('Epoch')\n",
    "axes[1, 1].axhline(y=20, color='r', linestyle='--', label='Target (~20)')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Final MSE: {history['mse_loss'][-1]:.4f}\")\n",
    "print(f\"Final L0: {history['l0_norm'][-1]:.2f} active features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Analyze Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze feature-affordance coupling\n",
    "analyzer = FeatureAffordanceAnalyzer(sae, dataset)\n",
    "\n",
    "# Compute correlations\n",
    "correlations = analyzer.compute_feature_affordance_correlation()\n",
    "top_features = analyzer.identify_top_features(n_top=20, method='correlation')\n",
    "\n",
    "print(\"Top 10 Features by Affordance Correlation:\")\n",
    "for idx, score in top_features[:10]:\n",
    "    print(f\"  Feature {idx:3d}: {score:.3f}\")\n",
    "\n",
    "# Visualize correlation distribution\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.hist(np.abs(correlations), bins=30, edgecolor='black')\n",
    "plt.xlabel('|Correlation| with Affordance Count')\n",
    "plt.ylabel('Number of Features')\n",
    "plt.title('Distribution of Feature-Affordance Correlations')\n",
    "plt.axvline(x=0.3, color='r', linestyle='--', label='Strong correlation (>0.3)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nFeatures with |correlation| > 0.3: {np.sum(np.abs(correlations) > 0.3)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine specific states and their features\n",
    "sample_idx = 10\n",
    "sample = dataset[sample_idx]\n",
    "\n",
    "state_tensor = sample['state'].unsqueeze(0)\n",
    "with torch.no_grad():\n",
    "    features = sae.encode(state_tensor).squeeze().numpy()\n",
    "    reconstruction = sae.decode(sae.encode(state_tensor)).squeeze().numpy()\n",
    "\n",
    "print(f\"Sample State {sample_idx}:\")\n",
    "print(f\"  Affordance cardinality: {sample['cardinality'].item():.0f}\")\n",
    "print(f\"  Active features (>0.01): {np.sum(features > 0.01)}\")\n",
    "print(f\"  Top 5 feature activations:\")\n",
    "top_5_idx = np.argsort(features)[-5:][::-1]\n",
    "for idx in top_5_idx:\n",
    "    print(f\"    Feature {idx}: {features[idx]:.3f}\")\n",
    "\n",
    "# Plot original vs reconstruction\n",
    "plt.figure(figsize=(12, 4))\n",
    "x = np.arange(len(sample['state']))\n",
    "plt.plot(x, sample['state'].numpy(), label='Original', alpha=0.7)\n",
    "plt.plot(x, reconstruction, label='Reconstruction', alpha=0.7)\n",
    "plt.xlabel('State Dimension')\n",
    "plt.ylabel('Value')\n",
    "plt.title('State Reconstruction Quality')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Test RL Verifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize verifiers\n",
    "verifiers = StoryWorldVerifiers(\n",
    "    min_encounters=3,\n",
    "    min_characters=2,\n",
    "    min_endings=2\n",
    ")\n",
    "\n",
    "# Test on synthetic storyworlds\n",
    "test_rewards = []\n",
    "for sw in storyworlds[:10]:\n",
    "    text = json.dumps(sw)\n",
    "    rewards = verifiers.compute_total_reward(text)\n",
    "    test_rewards.append(rewards)\n",
    "\n",
    "# Visualize reward components\n",
    "component_names = ['valid_json', 'schema', 'structure', 'effects', 'secrets', 'endings']\n",
    "component_means = {k: np.mean([r[k] for r in test_rewards]) for k in component_names}\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(component_names, component_means.values())\n",
    "plt.ylabel('Average Score')\n",
    "plt.title('Verifier Component Scores on Synthetic Data')\n",
    "plt.xticks(rotation=45)\n",
    "plt.ylim(0, 1)\n",
    "plt.grid(True, axis='y')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nAverage Scores:\")\n",
    "for k, v in component_means.items():\n",
    "    print(f\"  {k}: {v:.3f}\")\n",
    "print(f\"  Total: {np.mean([r['total'] for r in test_rewards]):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Mini RL Training (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Warning: This cell requires GPU and will take several minutes\n",
    "# Uncomment to run\n",
    "\n",
    "# config = RLConfig(\n",
    "#     model_name=\"gpt2\",\n",
    "#     max_length=512,\n",
    "#     batch_size=2,\n",
    "#     n_epochs=2,\n",
    "#     n_samples_per_epoch=10,\n",
    "#     learning_rate=5e-6\n",
    "# )\n",
    "\n",
    "# trainer = StoryWorldRLTrainer(config)\n",
    "# trainer.train()\n",
    "\n",
    "# # Test generation\n",
    "# sample = trainer.generate_storyworld()\n",
    "# print(sample[:300])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Feature-Aware Rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test feature-aware verifiers\n",
    "extractor = StoryWorldStateExtractor()\n",
    "feature_verifiers = FeatureAwareVerifiers(\n",
    "    sae=sae,\n",
    "    extractor=extractor,\n",
    "    min_encounters=3,\n",
    "    min_characters=2\n",
    ")\n",
    "\n",
    "# Compare base vs feature-aware rewards\n",
    "base_rewards = []\n",
    "feature_rewards = []\n",
    "\n",
    "for sw in storyworlds[:5]:\n",
    "    text = json.dumps(sw)\n",
    "    \n",
    "    base_r = verifiers.compute_total_reward(text)\n",
    "    feat_r = feature_verifiers.compute_total_reward(text)\n",
    "    \n",
    "    base_rewards.append(base_r['total'])\n",
    "    feature_rewards.append(feat_r['total'])\n",
    "    \n",
    "    print(f\"Storyworld {len(base_rewards)}:\")\n",
    "    print(f\"  Base reward: {base_r['total']:.3f}\")\n",
    "    print(f\"  Feature-aware: {feat_r['total']:.3f}\")\n",
    "    print(f\"  Feature quality: {feat_r['feature_quality']:.3f}\")\n",
    "    print()\n",
    "\n",
    "# Visualize comparison\n",
    "x = np.arange(len(base_rewards))\n",
    "width = 0.35\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(x - width/2, base_rewards, width, label='Base Reward')\n",
    "plt.bar(x + width/2, feature_rewards, width, label='Feature-Aware Reward')\n",
    "plt.xlabel('Storyworld')\n",
    "plt.ylabel('Reward')\n",
    "plt.title('Base vs Feature-Aware Rewards')\n",
    "plt.legend()\n",
    "plt.grid(True, axis='y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Save Models and Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save SAE\n",
    "save_dir = Path(\"./notebook_outputs\")\n",
    "save_dir.mkdir(exist_ok=True)\n",
    "\n",
    "torch.save(sae.state_dict(), save_dir / \"sae_notebook.pt\")\n",
    "\n",
    "# Save metrics\n",
    "import pickle\n",
    "with open(save_dir / \"training_history.pkl\", 'wb') as f:\n",
    "    pickle.dump(history, f)\n",
    "\n",
    "with open(save_dir / \"feature_correlations.pkl\", 'wb') as f:\n",
    "    pickle.dump(correlations, f)\n",
    "\n",
    "print(f\"Models and results saved to {save_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated:\n",
    "\n",
    "1. ✅ Synthetic storyworld generation\n",
    "2. ✅ Rollout collection\n",
    "3. ✅ SAE training on narrative states\n",
    "4. ✅ Feature-affordance analysis\n",
    "5. ✅ Multi-component verifier testing\n",
    "6. ✅ Feature-aware reward computation\n",
    "\n",
    "Next steps:\n",
    "- Scale to larger datasets\n",
    "- Run full RL training\n",
    "- Integrate with QFT-MCP corpus\n",
    "- Deploy production pipeline"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
